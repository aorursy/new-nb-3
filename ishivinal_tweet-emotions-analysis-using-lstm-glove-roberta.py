import preprocessor as p

import numpy as np 

import pandas as pd 

import emoji

import keras

from sklearn.model_selection import train_test_split

import tensorflow as tf

from keras.models import Sequential

from keras.layers.recurrent import LSTM, GRU,SimpleRNN

from keras.layers.core import Dense, Activation, Dropout

from keras.layers.embeddings import Embedding

from keras.layers.normalization import BatchNormalization

from keras.utils import np_utils

from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline

from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D

from keras.preprocessing import sequence, text

from keras.callbacks import EarlyStopping

from sklearn.preprocessing import LabelEncoder,OneHotEncoder

import plotly.graph_objects as go

import plotly.express as px

from tensorflow.keras.layers import Dense, Input

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.models import Model

from tensorflow.keras.callbacks import ModelCheckpoint

from kaggle_datasets import KaggleDatasets

import transformers

from transformers import TFAutoModel, AutoTokenizer

from tqdm.notebook import tqdm

from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors

from tqdm import tqdm
#data = pd.read_csv("/kaggle/input/tweet-sentiment-extraction/train.csv")

#data2 = pd.read_csv("/kaggle/input/tweet-sentiment-extraction/test.csv")
data = pd.read_csv("/kaggle/input/figure-eight-labelled-textual-dataset/text_emotion.csv")
misspell_data = pd.read_csv("/kaggle/input/spelling/aspell.txt",sep=":",names=["correction","misspell"])

misspell_data.misspell = misspell_data.misspell.str.strip()

misspell_data.misspell = misspell_data.misspell.str.split(" ")

misspell_data = misspell_data.explode("misspell").reset_index(drop=True)

misspell_data.drop_duplicates("misspell",inplace=True)

miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))



#Sample of the dict

{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}
def misspelled_correction(val):

    for x in val.split(): 

        if x in miss_corr.keys(): 

            val = val.replace(x, miss_corr[x]) 

    return val



data["clean_content"] = data.content.apply(lambda x : misspelled_correction(x))
contractions = pd.read_csv("/kaggle/input/contractions/contractions.csv")

cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))
def cont_to_meaning(val): 

  

    for x in val.split(): 

        if x in cont_dic.keys(): 

            val = val.replace(x, cont_dic[x]) 

    return val

data.clean_content = data.clean_content.apply(lambda x : cont_to_meaning(x))
p.set_options(p.OPT.MENTION, p.OPT.URL)

p.clean("hello guys @alx #sportüî• 1245 https://github.com/s/preprocessor")
data["clean_content"]=data.content.apply(lambda x : p.clean(x))
def punctuation(val): 

  

    punctuations = '''()-[]{};:'"\,<>./@#$%^&_~'''

  

    for x in val.lower(): 

        if x in punctuations: 

            val = val.replace(x, " ") 

    return val

punctuation("test @ #ldfldlf??? !! ")
data.clean_content = data.clean_content.apply(lambda x : ' '.join(punctuation(emoji.demojize(x)).split()))
def clean_text(val):

    val = misspelled_correction(val)

    val = cont_to_meaning(val)

    val = p.clean(val)

    val = ' '.join(punctuation(emoji.demojize(val)).split())

    

    return val
clean_text("isn't üí° adultry @ttt good bad ... ! ? ")
data = data[data.clean_content != ""]
data.sentiment.value_counts()
sent_to_id  = {"empty":0, "sadness":1,"enthusiasm":2,"neutral":3,"worry":4,

                        "surprise":5,"love":6,"fun":7,"hate":8,"happiness":9,"boredom":10,"relief":11,"anger":12}
data["sentiment_id"] = data['sentiment'].map(sent_to_id)

data
label_encoder = LabelEncoder()

integer_encoded = label_encoder.fit_transform(data.sentiment_id)



onehot_encoder = OneHotEncoder(sparse=False)

integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)

Y = onehot_encoder.fit_transform(integer_encoded)
X_train, X_test, y_train, y_test = train_test_split(data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)
# using keras tokenizer here

token = text.Tokenizer(num_words=None)

max_len = 160

Epoch = 5

token.fit_on_texts(list(X_train) + list(X_test))

X_train_pad = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_len)

X_test_pad = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_len)
w_idx = token.word_index
embed_dim = 160

lstm_out = 250



model = Sequential()

model.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1]))

model.add(SpatialDropout1D(0.2))

model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))

model.add(keras.layers.core.Dense(13, activation='softmax'))

#adam rmsprop 

model.compile(loss = "categorical_crossentropy", optimizer='adam',metrics = ['accuracy'])

print(model.summary())
batch_size = 32
model.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))
def get_sentiment(model,text):

    text = clean_text(text)

    #tokenize

    twt = token.texts_to_sequences([text])

    twt = sequence.pad_sequences(twt, maxlen=max_len, dtype='int32')

    sentiment = model.predict(twt,batch_size=1,verbose = 2)

    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]

    result = pd.DataFrame([sent_to_id.keys(),sent]).T

    result.columns = ["sentiment","percentage"]

    result=result[result.percentage !=0]

    return result
def plot_result(df):

    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']

    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])

    #fig.show()

    colors={'love':'rgb(213,0,0)','empty':'rgb(0,0,0)',

                    'sadness':'rgb(0,142,248)','enthusiasm':'rgb(245,178,123)',

                    'neutral':'rgb(237,236,236)','worry':'rgb(216,74,9)',

                    'surprise':'rgb(1,155,189)','fun':'rgb(255,208,0)',

                    'hate':'rgb(120,0,160)','happiness':'rgb(9,143,69)',

                    'boredom':'rgb(128,124,124)','relief':'rgb(133,221,233)',

                    'anger':'rgb(245,94,16)'}

    col_2={}

    for i in result.sentiment.to_list():

        col_2[i]=colors[i]

    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)

    fig.show()
result =get_sentiment(model,"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing")

plot_result(result)

result =get_sentiment(model,"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you")

plot_result(result)

result =get_sentiment(model,"I hate this game so much,It make me angry all the time ")

plot_result(result)
def read_data(file_name):

    with open(file_name,'r') as f:

        word_vocab = set() 

        word2vector = {}

        for line in f:

            line_ = line.strip() 

            words_Vec = line_.split()

            word_vocab.add(words_Vec[0])

            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)

    print("Total Words in DataSet:",len(word_vocab))

    return word_vocab,word2vector
vocab, word_to_idx =read_data("/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt")
embedding_matrix = np.zeros((len(w_idx) + 1, 200))

for word, i in w_idx.items():

    embedding_vector = word_to_idx.get(word)

    if embedding_vector is not None:

        embedding_matrix[i] = embedding_vector
embed_dim = 200

lstm_out = 250



model_lstm_gwe = Sequential()

model_lstm_gwe.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1],weights=[embedding_matrix],trainable=False))

model_lstm_gwe.add(SpatialDropout1D(0.2))

model_lstm_gwe.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))

model_lstm_gwe.add(keras.layers.core.Dense(13, activation='softmax'))

#adam rmsprop 

model_lstm_gwe.compile(loss = "categorical_crossentropy", optimizer='adam',metrics = ['accuracy'])

print(model_lstm_gwe.summary())
batch_size = 32


model_lstm_gwe.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))
result =get_sentiment(model_lstm_gwe,"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing")

plot_result(result)

result =get_sentiment(model_lstm_gwe,"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you")

plot_result(result)

result =get_sentiment(model_lstm_gwe,"I hate this game so much,It make me angry all the time ")

plot_result(result)
def regular_encode(texts, tokenizer, maxlen=512):

    enc_di = tokenizer.batch_encode_plus(

        texts, 

        return_attention_masks=False, 

        return_token_type_ids=False,

        pad_to_max_length=True,

        max_length=maxlen

    )

    

    return np.array(enc_di['input_ids'])



def build_model(transformer, max_len=160):

    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")

    sequence_output = transformer(input_word_ids)[0]

    cls_token = sequence_output[:, 0, :]

    out = Dense(13, activation='softmax')(cls_token)

    

    model = Model(inputs=input_word_ids, outputs=out)

    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

    

    return model
AUTO = tf.data.experimental.AUTOTUNE

MODEL = 'roberta-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL)
X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)

X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)
train_dataset = (

    tf.data.Dataset

    .from_tensor_slices((X_train_t, y_train))

    .repeat()

    .shuffle(1995)

    .batch(batch_size)

    .prefetch(AUTO)

)



valid_dataset = (

    tf.data.Dataset

    .from_tensor_slices((X_test_t, y_test))

    .batch(batch_size)

    .cache()

    .prefetch(AUTO)

)

transformer_layer = TFAutoModel.from_pretrained(MODEL)

model_roberta_base = build_model(transformer_layer, max_len=max_len)

model_roberta_base.summary()
n_steps = X_train.shape[0] // batch_size

model_roberta_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)
def get_sentiment2(model,text):

    text = clean_text(text)

    #tokenize

    x_test1 = regular_encode([text], tokenizer, maxlen=max_len)

    test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))

    #test1

    sentiment = model.predict(test1,verbose = 0)

    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]

    result = pd.DataFrame([sent_to_id.keys(),sent]).T

    result.columns = ["sentiment","percentage"]

    result=result[result.percentage !=0]

    return result
result =get_sentiment2(model_roberta_base,"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing")

plot_result(result)

result =get_sentiment2(model_roberta_base,"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you")

plot_result(result)

result =get_sentiment2(model_roberta_base,"I hate this game so much,It make me angry all the time ")

plot_result(result)
AUTO = tf.data.experimental.AUTOTUNE

MODEL = 'albert-base-v2'

tokenizer = AutoTokenizer.from_pretrained(MODEL)

X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)

X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)
train_dataset = (

    tf.data.Dataset

    .from_tensor_slices((X_train_t, y_train))

    .repeat()

    .shuffle(1995)

    .batch(batch_size)

    .prefetch(AUTO)

)



valid_dataset = (

    tf.data.Dataset

    .from_tensor_slices((X_test_t, y_test))

    .batch(batch_size)

    .cache()

    .prefetch(AUTO)

)
transformer_layer = TFAutoModel.from_pretrained(MODEL)

albert = build_model(transformer_layer, max_len=max_len)

albert.summary()
n_steps = X_train.shape[0] // batch_size

albert.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)
result =get_sentiment2(albert,"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing")

plot_result(result)

result =get_sentiment2(albert,"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you")

plot_result(result)

result =get_sentiment2(albert,"I hate this game so much,It make me angry all the time ")

plot_result(result)